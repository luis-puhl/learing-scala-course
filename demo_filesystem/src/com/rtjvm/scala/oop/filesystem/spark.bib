@inproceedings{Dean2004,
abstract = {MapReduce is a programming model and an associated implementation for processing and generating large datasets that is amenable to a broad variety of real-world tasks. Users specify the computation in terms of a map and a reduce function, and the underlying runtime system automatically parallelizes the computation across large-scale clusters of machines, handles machine failures, and schedules inter-machine communication to make efficient use of the network and disks. Programmers find the system easy to use: more than ten thousand distinct MapReduce programs have been implemented internally at Google over the past four years, and an average of one hundred thousand MapReduce jobs are executed on Google's clusters every day, processing a total of more than twenty petabytes of data per day.},
author = {Dean, Jeffrey and Ghemawat, Sanjay},
booktitle = {USENIX Association OSDI '04: 6th Symposium on Operating Systems Design and Implementation},
doi = {10.1145/1327452.1327492},
file = {:home/puhl/mendeley/Dean, Ghemawat_2004_MapReduce Simplified data processing on large clusters.pdf:pdf},
issn = {00010782},
number = {1},
pages = {107--113},
title = {{MapReduce: Simplified data processing on large clusters}},
volume = {51},
year = {2004}
}
@article{Zaharia,
abstract = {MapReduce and its variants have been highly successful in implementing large-scale data-intensive applications on commodity clusters. However; Spark introduces an abstraction called resilient distributed datasets (RDDs). An RDD is a read-only collection of objects partitioned across a set of machines that can be rebuilt if a partition is lost. Spark can outperform Hadoop by 10x in iterative machine learning jobs; and can be used to interactively query a 39 GB dataset with sub-second response time.; as well as interactive data analysis tools. We propose a new framework called Spark that supports these applications while retaining the scalability and fault tolerance of MapReduce. To achieve these goals; most of these systems are built around an acyclic data flow model that is not suitable for other popular applications. This paper focuses on one such class of applications: those that reuse a working set of data across multiple parallel operations. This includes many iterative machine learning algorithms},
author = {Zaharia, Matei and Chowdhury, Mosharaf and Franklin, Michael J and Shenker, Scott and Stoica, Ion},
file = {:home/puhl/mendeley/Zaharia et al._2010_Spark Cluster computing with working sets.pdf:pdf},
journal = {HotCloud},
number = {10-10},
pages = {95},
title = {{Spark: Cluster computing with working sets.}},
volume = {10},
year = {2010}
}
